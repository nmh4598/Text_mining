{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#%reset\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import tweepy\n",
    "import sqlalchemy\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from unidecode import unidecode\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récolter les données via l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=\"\")\n",
    "\n",
    "CAC40 = ['Air liquide', \n",
    "         'Airbus',\n",
    "         'Alstom',\n",
    "         'ArcelorMittal',\n",
    "         'Axa',\n",
    "         'BNP Paribas',\n",
    "         'Bouygues',\n",
    "         'Capgemini',\n",
    "         'Carrefour',\n",
    "         'Crédit agricole',\n",
    "         'Danone',\n",
    "         'Dassault Systèmes',\n",
    "         'Engie',\n",
    "         'EssilorLuxotica',\n",
    "         'Eurofins Scientific',\n",
    "         'Hermès International',\n",
    "         'Kering',\n",
    "         'Legrand',\n",
    "         \"L'Oréal\",\n",
    "         \"LVMH\",\n",
    "         'Michelin',\n",
    "         'Orange',\n",
    "         'Pernod Ricard',\n",
    "         'Publicis Groupe',\n",
    "         'Renault',\n",
    "         'Safran',\n",
    "         'Saint-Gobain',\n",
    "         'Sanofi',\n",
    "         'Schneider Electric',\n",
    "         'Société générale',\n",
    "         'Stellantis',\n",
    "         'STMicroelectronics',\n",
    "         'Teleperformance',\n",
    "         'Thales',\n",
    "         'TotalEnergies',\n",
    "         'Unibail-Rodamco-Westfield',\n",
    "         'Veolia',\n",
    "         'Vinci',\n",
    "         'Vivendi',\n",
    "         'Worldline']\n",
    "\n",
    "\n",
    "for entreprise in CAC40 :\n",
    "    query = entreprise + ' -is:retweet lang:fr'\n",
    "\n",
    "    tweets= tweepy.Paginator(client.search_recent_tweets, query=query,\n",
    "                              tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=5000)\n",
    "\n",
    "    df_id = []\n",
    "    df_tweet = []\n",
    "    df_date = []\n",
    "    df_entreprise = []\n",
    "    for tweet in tweets:\n",
    "       df_id.append(tweet.id)\n",
    "       df_tweet.append(tweet.text)\n",
    "       df_date.append(tweet.created_at)\n",
    "       df_entreprise.append(entreprise)\n",
    "    list_of_tuples = list(zip(df_id , df_date, df_tweet, df_entreprise))\n",
    "    df = pd.DataFrame(list_of_tuples, columns=['id', 'date', 'text', 'entreprise'])\n",
    "# Sauvegarder les données dans la base des données \n",
    "    URI = \"mysql://root:master2@localhost/textmining\"\n",
    "    con= sqlalchemy.create_engine(URI)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], infer_datetime_format=True)\n",
    "    df.to_sql(name=\"tweet\", con=con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connexion a la BDD\n",
    "URI = \"mysql://root:master2@localhost/textmining\"\n",
    "con = sqlalchemy.create_engine(URI)\n",
    "\n",
    "data = pd.read_sql(\"SELECT * FROM  tweet\", con=con)\n",
    "path_input = Path.cwd().parent.joinpath('data')\n",
    "\n",
    "# Sauvegarder les données sous forme parquet \n",
    "path_data_input = path_input.joinpath('data_init.parquet')\n",
    "data.to_parquet(path_data_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class pour nettoyer les données et analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.entreprises = [\"LVMH\", \"L'Oréal\", \"Hermès\", \"TotalEnergies\", \n",
    "                            \"Sanofi\", \"Airbus\", \"Schneider Electric\", \"Capgemini\", \n",
    "                            \"Air liquide\", \"BNP Paribas\"]\n",
    " \n",
    "    def conservation_top_10(self):\n",
    "        \"\"\"\n",
    "            Conservation des 10 plus grosses entreprises du CAC40\n",
    "        \"\"\"\n",
    "        self.data = self.data[self.data.entreprise.isin(self.entreprises)]\n",
    "        self.n_tweet = len(self.data)\n",
    "\n",
    "    def supprime_urls_hashtags_mentions_ponctuations_nombres(self):\n",
    "        \"\"\"\n",
    "        Supprime les liens https jusqu'à l'espace suivant\n",
    "        Supprime toutes les mentions (@) ainsi que la suite jusqu'à l'espace suivant\n",
    "        Supprime tous les Hashtags (#) ainsi que la suite jusqu'à l'espace suivant \n",
    "        Supprime tous les nombres\n",
    "        Supprime tous les pontuations\n",
    "        Supprime tous les accents\n",
    "        \"\"\"\n",
    "        url_regex = r'(http[s]?:\\/\\/\\S+)'\n",
    "        pattern = r'(@\\w+|#\\w+|http[s]?:\\/\\/\\S+)'\n",
    "        num_regex = r'\\d+'\n",
    "        \n",
    "        def clean(text):\n",
    "            items = re.findall(pattern, text)\n",
    "            mentions = [item for item in items if item.startswith('@')]\n",
    "            hashtags = [item for item in items if item.startswith('#')]\n",
    "            urls = [item for item in items if re.match(url_regex, item)]         \n",
    "            n_mentions = len(mentions)\n",
    "            n_hashtags = len(hashtags)\n",
    "            n_urls = len(urls)    \n",
    "            text = re.sub(pattern, '', text)\n",
    "            text = re.sub(num_regex, '', text)\n",
    "            punctuations = re.findall(r'[^\\w\\s]', text)  \n",
    "            n_pontuations = len(punctuations)\n",
    "            # Ice, faut remplace par l'espace \n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "                \n",
    "            return text, mentions, hashtags, urls, punctuations\n",
    "\n",
    "        self.data['text'], self.data['mentions'], self.data['hashtags'], self.data['urls'], self.data['punctuations']= zip(*self.data['text'].apply(clean))\n",
    "        self.data['text'] = self.data['text'].str.lower()\n",
    "\n",
    "        def remove_accents(text):\n",
    "            return unidecode(text)\n",
    "        self.data['text'] = self.data['text'].apply(remove_accents)\n",
    "\n",
    "    def supprime_les_stopwords(self):\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "        liste_mot_non_voulu = [\"lvmh\", \"oreal\",\"oréal\",\"L’Oréal\", \"sanofi\", \"airbus\", \"totalenergie\", \"totalenergies\", \"total\", \"energie\", \"energies\", \"air\", \"liquide\", \"bnp\", \"paribas\", \"pariba\", \"airliquide\", \"ça\", \"ca\", \"faire\", \"moi\", \"deja\"]\n",
    "        list_sw = stop_words_txt[\"vides\"].to_list()\n",
    "        stop_words.update(liste_mot_non_voulu)\n",
    "        stop_words.update(list_sw)\n",
    "        \n",
    "        def supprime_stopwords(text):\n",
    "            words = re.split(r\"[ ’'\\-]\", text)\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_stop_words = [word for word in words if word.lower() in stop_words]\n",
    "            return ' '.join(filtered_words),filtered_stop_words\n",
    "\n",
    "        self.data['text'],self.data[\"stopword\"]= zip(*self.data['text'].apply(supprime_stopwords))\n",
    "    \n",
    "    def transform_normal(self):\n",
    "        \"\"\"       \n",
    "            Supprime tous les accents     \n",
    "            Supprime les tweets n'ayant plus de texte. \n",
    "        \"\"\"\n",
    "        self.data['text'] = self.data['text'].str.strip()\n",
    "        self.data = self.data[self.data['text'] != '']\n",
    "        self.data['date'] = pd.to_datetime(self.data['date']).dt.date\n",
    "        \n",
    "    def supprime_doublons(self):\n",
    "        \"\"\"\n",
    "            Supprime tous les doublons avec le même texte\n",
    "        \"\"\"\n",
    "        self.data = self.data.drop_duplicates([\"text\"])\n",
    "    \n",
    "    def lemmatize(self, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if token.pos_ in allowed_postags]))\n",
    "        \n",
    "    def sentiment_analysis(self):\n",
    "        def getSubjectivity(text):\n",
    "            \"\"\" \n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la \n",
    "                forme (subjectivité), où la subjectivité est un flottant compris entre 0,0 \n",
    "                et 1,0, indiquant la subjectivité du texte. Une subjectivité de 0,0 signifie \n",
    "                que le texte est très objectif et factuel, tandis qu'une subjectivité de \n",
    "                1,0 signifie que le texte est très subjectif et opiniâtre.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "\n",
    "        def getPolarity(text):\n",
    "            \"\"\"\n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la forme \n",
    "                (polarity ), où polarity est un flottant compris entre -1.0 et 1.0, indiquant\n",
    "                la polarité de sentiment du texte. Une polarité de -1,0 est très négative,\n",
    "                0 est neutre et 1,0 est très positive.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "\n",
    "        self.data[\"Subjectivité\"] = self.data[\"text\"].apply(getSubjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"text\"].apply(getPolarity)\n",
    "\n",
    "\n",
    "        def getAnalysis_polarity(score):\n",
    "            if score < 0:\n",
    "                return \"Negative\"\n",
    "            elif score == 0:\n",
    "                return \"Neutral\"\n",
    "            else:\n",
    "                return \"Positive\"\n",
    "        def getAnalysis_subjectivity(score):\n",
    "            if score < 0.5:\n",
    "                return \"Très objectif et factuel\"\n",
    "            else:\n",
    "                return \"Très subjectif et opiniâtre\"\n",
    "        self.data[\"Subjectivité\"] = self.data[\"Subjectivité\"].apply(getAnalysis_subjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"Polarité\"].apply(getAnalysis_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):  \n",
    "    data_processor = DataProcessor(data)\n",
    "    data_processor.conservation_top_10()\n",
    "    data_processor.supprime_urls_hashtags_mentions_ponctuations_nombres()\n",
    "    data_processor.lemmatize() \n",
    "    data_processor.supprime_les_stopwords()\n",
    "    data_processor.transform_normal()\n",
    "    data_processor.supprime_doublons()\n",
    "    data_processor.sentiment_analysis()\n",
    "    return data_processor.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m stop_words_txt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(path_stopwords, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m data_init \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(path_data_input) \n\u001b[1;32m----> 9\u001b[0m data_fin \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m(data_init)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "path_input = Path.cwd().parent.joinpath('data')\n",
    "\n",
    "path_data_input = path_input.joinpath('data_init.parquet')\n",
    "path_stopwords = path_input.joinpath('stop_words_french.txt')\n",
    "\n",
    "stop_words_txt = pd.read_table(path_stopwords, header = 0)\n",
    "data_init = pd.read_parquet(path_data_input) \n",
    "\n",
    "data_fin = transform(data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input = Path.cwd().parent.joinpath('data')\n",
    "\n",
    "path_data_input = path_input.joinpath('data_init.parquet')\n",
    "path_stopwords = path_input.joinpath('stop_words_french.txt')\n",
    "\n",
    "stop_words_txt = pd.read_table(path_stopwords, header = 0)\n",
    "data_init = pd.read_parquet(path_data_input) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarder les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = Path.cwd().parent.joinpath('outputs')\n",
    "\n",
    "path_data_output = path_output.joinpath('data_fin.parquet')\n",
    "\n",
    "data_fin.to_parquet(path_data_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0efa8c74aad001ef25a6d80deef3616d04c277939308e305a946d7ec788eb1bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
